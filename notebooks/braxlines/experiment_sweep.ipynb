{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "# Sweep Training\n",
        "\n",
        "We can perform hyperparameter sweep directly on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYe1kc3a4Oxc"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/brax/blob/main/notebooks/braxlines/experiment_sweep.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlVNS8JstMRr"
      },
      "outputs": [],
      "source": [
        "#@title Colab setup and imports\n",
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a TPU runtime.  From the Colab menu, choose Runtime \u003e Change Runtime Type, then select **'TPU'** in the dropdown.\n",
        "\n",
        "#@markdown See [config_utils.py](https://github.com/google/brax/blob/main/brax/experimental/braxlines/common/config_utils.py)\n",
        "#@markdown for the configuration format.\n",
        "from datetime import datetime\n",
        "import importlib\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "from IPython.display import HTML, clear_output\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "agent = 'vgcrl' # @param ['vgcrl', 'irl_smm', 'composer']\n",
        "if agent == 'composer':\n",
        "  agent_module = f'brax.experimental.composer.train'\n",
        "else:\n",
        "  agent_module = f'brax.experimental.braxlines.{agent}.train'\n",
        "output_path = '' #@param{'type': 'string'}\n",
        "start_count = 0 # @param{'type': 'integer'}\n",
        "end_count = 100000000 # @param{'type': 'integer'}\n",
        "experiment_path = '' #@param{'type': 'string'}\n",
        "experiment_path=experiment_path or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_path = f'{output_path}/{experiment_path}'\n",
        "\n",
        "from brax.experimental.braxlines.common import config_utils\n",
        "train = importlib.import_module(agent_module)\n",
        "\n",
        "if agent == 'composer':\n",
        "  config = [\n",
        "    dict(\n",
        "        env_name = ['ant_push'],\n",
        "        desc_edits = {\n",
        "          'components.cap1.reward_fns.goal.scale': [0.2, 1, 0.5],\n",
        "          'components.cap1.reward_fns.goal.target_goal': 5, \n",
        "        },\n",
        "        ppo_params = dict(\n",
        "          num_timesteps=int(2.5 * 1e8),\n",
        "          reward_scaling=10,\n",
        "          episode_length=1000,\n",
        "          normalize_observations=True,\n",
        "          action_repeat=1,\n",
        "          unroll_length=5,\n",
        "          num_minibatches=32,\n",
        "          num_update_epochs=4,\n",
        "          discounting=0.95,\n",
        "          learning_rate=3e-4,\n",
        "          entropy_cost=1e-2,\n",
        "          num_envs=2048,\n",
        "          batch_size=1024,)\n",
        "    ),\n",
        "  ]\n",
        "elif agent == 'vgcrl':\n",
        "  config = [\n",
        "    dict(\n",
        "        env_name = ['ant'],\n",
        "        obs_indices = 'vel',\n",
        "        algo_name = ['gcrl', 'diayn', 'cdiayn', 'diayn_full'],\n",
        "        obs_scale = [5.0],\n",
        "        seed = [0],\n",
        "        normalize_obs_for_disc = False,\n",
        "        evaluate_mi = True,\n",
        "        evaluate_lgr = False,\n",
        "        env_reward_multiplier = 0.0,\n",
        "        spectral_norm = [True],\n",
        "        ppo_params = dict(\n",
        "          num_timesteps=int(2.5 * 1e8),\n",
        "          reward_scaling=10,\n",
        "          episode_length=1000,\n",
        "          normalize_observations=True,\n",
        "          action_repeat=1,\n",
        "          unroll_length=5,\n",
        "          num_minibatches=32,\n",
        "          num_update_epochs=4,\n",
        "          discounting=0.95,\n",
        "          learning_rate=3e-4,\n",
        "          entropy_cost=1e-2,\n",
        "          num_envs=2048,\n",
        "          batch_size=1024,)\n",
        "    ),\n",
        "  ]\n",
        "elif agent == 'irl_smm':\n",
        "  config = [\n",
        "  dict(\n",
        "      env_name = ['ant'],\n",
        "      obs_indices = 'vel',\n",
        "      target_num_modes = [2],\n",
        "      obs_scale = [8], \n",
        "      reward_type = ['gail2', 'mle', 'airl'],\n",
        "      seed = [0],\n",
        "      normalize_obs_for_disc = False,\n",
        "      evaluate_dist =False,\n",
        "      env_reward_multiplier = 0.0,\n",
        "      spectral_norm = [True],\n",
        "      ppo_params = dict(\n",
        "        num_timesteps=int(1.5 * 1e8),\n",
        "        reward_scaling=10,\n",
        "        episode_length=1000,\n",
        "        normalize_observations=True,\n",
        "        action_repeat=1,\n",
        "        unroll_length=5,\n",
        "        num_minibatches=32,\n",
        "        num_update_epochs=4,\n",
        "        discounting=0.95,\n",
        "        learning_rate=3e-4,\n",
        "        entropy_cost=1e-2,\n",
        "        num_envs=2048,\n",
        "        batch_size=1024,)\n",
        "  ),\n",
        "]\n",
        "\n",
        "\n",
        "prefix_keys = config_utils.list_keys_to_expand(config)\n",
        "for c, p in zip(config, prefix_keys):\n",
        "  c.update(dict(prefix_keys=p))\n",
        "config_count= config_utils.count_configuration(config)\n",
        "start_count= max(start_count, 0)\n",
        "end_count = min(end_count, sum(config_count))\n",
        "print(f'Loaded agent_module={agent_module}')\n",
        "print(f'Loaded {sum(config_count)}({config_count}) experiment configurations')\n",
        "print(f'Set start_count={start_count}, end_count={end_count}')\n",
        "print(f'Set prefix_keys={prefix_keys}')\n",
        "print(f'Set output_dir={output_path}')\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaJDZqhCLovU"
      },
      "outputs": [],
      "source": [
        "#@title Launch experiments\n",
        "ignore_errors = False # @param{'type': 'boolean'}\n",
        "\n",
        "return_dict = {}\n",
        "for i in range(start_count, end_count):\n",
        "  c, _= config_utils.index_configuration(config, index=i, count=config_count)\n",
        "  task_name = config_utils.get_compressed_name_from_keys(\n",
        "      c, train.TASK_KEYS)\n",
        "  experiment_name = config_utils.get_compressed_name_from_keys(\n",
        "      c, c.pop('prefix_keys'))\n",
        "  output_dir = f'{output_path}/{task_name}/{experiment_name}'\n",
        "  print(f'[{i+1}/{sum(config_count)}] Starting experiment...')\n",
        "  print(f'\\t config: {pprint.pformat(c, indent=2)}')\n",
        "  print(f'\\t output_dir={output_dir}')\n",
        "  print(f'\\t previous time_to_jit={return_dict.get(\"time_to_train\", None)}')\n",
        "  print(f'\\t previous time_to_train={return_dict.get(\"time_to_jit\", None)}')\n",
        "  return_dict = {}\n",
        "  if ignore_errors:\n",
        "    try:\n",
        "      train.train(c, output_dir=output_dir, return_dict=return_dict)\n",
        "    except Exception as e:\n",
        "      print(f'[{i+1}/{sum(config_count)}] FAILED experiment {e.__class__.__name__}: {e.message}')\n",
        "  else:\n",
        "    train.train(c, output_dir=output_dir, return_dict=return_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "experiment_sweep.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1zvUdazhGU7ZjPl-Vb2GSESCWtEgiw2bJ",
          "timestamp": 1632316705245
        },
        {
          "file_id": "1ZaAO4BS2tJ_03CIXdBCFibZR2yLl6dtv",
          "timestamp": 1629608669428
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
